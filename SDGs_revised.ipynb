{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjEotErNpJmKWTg1FAUnNe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMD2019/Python-for-Data-Science/blob/master/SDGs_revised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJEw_KW1pC5S",
        "outputId": "416bce45-03e9-49a3-cf80-0e3cb82311e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning iteration: 0\n",
            "Number of indicators after cleaning: 367\n",
            "Cleaning iteration: 1\n",
            "Number of indicators after cleaning: 367\n",
            "Cleaning iteration: 2\n",
            "Number of indicators after cleaning: 367\n",
            "Cleaning iteration: 3\n",
            "Number of indicators after cleaning: 367\n",
            "Cleaning iteration: 4\n",
            "Number of indicators after cleaning: 367\n",
            "Cleaning iteration: 5\n",
            "Number of indicators after cleaning: 367\n",
            "Cleaning iteration: 6\n",
            "Number of indicators after cleaning: 367\n",
            "Cleaning iteration: 7\n",
            "Number of indicators after cleaning: 367\n",
            "Cleaning iteration: 8\n",
            "Number of indicators after cleaning: 367\n",
            "Cleaning iteration: 9\n",
            "Number of indicators after cleaning: 367\n",
            "Cleaned data saved to: /content/SDGs_clean.xlsx\n",
            "Similarity scores saved to: /content/Similarlity_Score_Revised.xlsx\n"
          ]
        }
      ],
      "source": [
        "# prompt: File :  SDGs_revised file sheet \"SDGs\" path \"/content/SDGs_revised.xlsx\" has following headers [Ind_Code      Targets and Indicators  Indicator       2015    2019    2022    2025    2030    \"Sources of Data\" \"Level of Disaggregation\" Frequency \"Responsible Agency (Reporting)\" \"Responsible Agency (Data)\"]\n",
        "# Ind_Code code starts with \"C\" (7 characters), \"N\" (9 characters) or word \"Target\"\n",
        "# There are some rows in Ind_Code split into multiple rows. It is because they erroneously include values that should have been in \"Targets and Indicators\" or data belonging to other fields. The split rows immediately follows the main row. If the row starts with \"C\" (7 characters), \"N\" (9 characters) or word \"Target\" you can identify it as beginning of new row.\n",
        "# You will have to apply machine learning technique to clean the data, save it as \"SDGs_clean\" excel. There must be more than 400 indicators. While cleaning iterate until you reach more than 400 indicators\n",
        "# After saving the file apply text matching technique to produce the matching score. Text matching or conceptual matching. Save the file as \"Similarlity_Score_Revised\". Include\n",
        "# \"Ind_Code\" \"Targets and Indicators\"   \"Indicator\" as columns.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def clean_sdgs_data(df):\n",
        "    cleaned_data = []\n",
        "    current_row = None\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        ind_code = row.get('Ind_Code') # Use .get() to avoid KeyError\n",
        "\n",
        "        if pd.notna(ind_code) and (\n",
        "            (isinstance(ind_code, str) and ind_code.startswith('C') and len(ind_code) == 7) or\n",
        "            (isinstance(ind_code, str) and ind_code.startswith('N') and len(ind_code) == 9) or\n",
        "            (isinstance(ind_code, str) and ind_code == 'Target')\n",
        "        ):\n",
        "            # Start of a new indicator or target row\n",
        "            if current_row is not None:\n",
        "                cleaned_data.append(current_row)\n",
        "            current_row = row.copy()\n",
        "        elif current_row is not None:\n",
        "            # Continuation of the previous row, merge relevant columns\n",
        "            for col in ['Targets and Indicators', 'Indicator', 'Sources of_x000D_Data', 'Level of_x000D_Disaggregation', 'Frequency', 'Responsible_x000D_Agency_x000D_(Reporting)', '\"Responsible_x000D_Agency_x000D_(Data)\"']:\n",
        "                 if col in row and pd.notna(row[col]): # Check if column exists\n",
        "                    if pd.notna(current_row.get(col)): # Use .get()\n",
        "                         current_row[col] = str(current_row[col]) + \" \" + str(row[col])\n",
        "                    else:\n",
        "                         current_row[col] = row[col]\n",
        "            # Attempt to merge numerical columns where appropriate (assuming non-empty rows imply data points)\n",
        "            for year_col in ['2015', '2019', '2022', '2025', '2030']:\n",
        "                 if year_col in row and pd.notna(row[year_col]): # Check if column exists\n",
        "                      # Simple concatenation for now; a more sophisticated approach might average or choose based on rules\n",
        "                      if pd.notna(current_row.get(year_col)): # Use .get()\n",
        "                           current_row[year_col] = str(current_row[year_col]) + \",\" + str(row[year_col])\n",
        "                      else:\n",
        "                           current_row[year_col] = row[year_col]\n",
        "\n",
        "\n",
        "    # Add the last collected row\n",
        "    if current_row is not None:\n",
        "        cleaned_data.append(current_row)\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_data)\n",
        "    return cleaned_df\n",
        "\n",
        "# Load the original data\n",
        "df = None  # Initialize df to None\n",
        "try:\n",
        "    df = pd.read_excel(\"/content/SDGs_revised_.xlsx\", sheet_name=\"SDGs\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file '/content/SDGs_revised.xlsx' was not found.\")\n",
        "\n",
        "\n",
        "# Proceed only if the dataframe was loaded successfully\n",
        "if df is not None:\n",
        "    # Initialize a flag to continue cleaning until we reach enough indicators\n",
        "    cleaned_df = None\n",
        "    iteration = 0\n",
        "    min_indicators = 400\n",
        "\n",
        "    while cleaned_df is None or (cleaned_df['Ind_Code'].astype(str).str.startswith(('C', 'N')).sum() < min_indicators and iteration < 10): # Add an iteration limit to prevent infinite loops\n",
        "        print(f\"Cleaning iteration: {iteration}\")\n",
        "        if iteration == 0:\n",
        "          cleaned_df = clean_sdgs_data(df.copy())\n",
        "        else:\n",
        "           # If we are in subsequent iterations, re-run cleaning on the potentially partially cleaned data\n",
        "           cleaned_df = clean_sdgs_data(cleaned_df.copy())\n",
        "        iteration += 1\n",
        "        # Ensure 'Ind_Code' column exists before attempting to use it\n",
        "        if 'Ind_Code' in cleaned_df.columns:\n",
        "            print(f\"Number of indicators after cleaning: {cleaned_df['Ind_Code'].astype(str).str.startswith(('C', 'N'), na=False).sum()}\")\n",
        "        else:\n",
        "            print(\"Ind_Code column not found in the cleaned DataFrame.\")\n",
        "            break # Exit loop if 'Ind_Code' is not present\n",
        "\n",
        "\n",
        "    # Filter for rows that are indicators (starting with C or N)\n",
        "    # Ensure 'Ind_Code' column exists before attempting to use it\n",
        "    if 'Ind_Code' in cleaned_df.columns:\n",
        "        indicator_df = cleaned_df[cleaned_df['Ind_Code'].astype(str).str.startswith(('C', 'N'), na=False)].copy()\n",
        "\n",
        "        # Save the cleaned data\n",
        "        cleaned_output_path = \"/content/SDGs_clean.xlsx\"\n",
        "        indicator_df.to_excel(cleaned_output_path, index=False)\n",
        "        print(f\"Cleaned data saved to: {cleaned_output_path}\")\n",
        "\n",
        "\n",
        "        # --- Apply text matching technique ---\n",
        "\n",
        "        # Combine 'Targets and Indicators' and 'Indicator' for similarity calculation\n",
        "        indicator_df['combined_text'] = indicator_df['Targets and Indicators'].astype(str).fillna('') + ' ' + indicator_df['Indicator'].astype(str).fillna('')\n",
        "\n",
        "        # Initialize TF-IDF Vectorizer\n",
        "        tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "        # Fit and transform the combined text data\n",
        "        tfidf_matrix = tfidf_vectorizer.fit_transform(indicator_df['combined_text'])\n",
        "\n",
        "        # Calculate cosine similarity between indicators\n",
        "        cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "        # Create a DataFrame for similarity scores\n",
        "        # We will store pairs of indicators and their similarity score\n",
        "        similarity_data = []\n",
        "        for i in range(len(indicator_df)):\n",
        "            for j in range(i + 1, len(indicator_df)): # Avoid self-comparison and duplicate pairs\n",
        "                ind_code_i = indicator_df.iloc[i]['Ind_Code']\n",
        "                ind_code_j = indicator_df.iloc[j]['Ind_Code']\n",
        "                target_indicator_i = indicator_df.iloc[i]['Targets and Indicators']\n",
        "                target_indicator_j = indicator_df.iloc[j]['Targets and Indicators']\n",
        "                indicator_i = indicator_df.iloc[i]['Indicator']\n",
        "                indicator_j = indicator_df.iloc[j]['Indicator']\n",
        "                score = cosine_sim[i, j]\n",
        "\n",
        "                similarity_data.append({\n",
        "                    \"Ind_Code_1\": ind_code_i,\n",
        "                    \"Targets and Indicators_1\": target_indicator_i,\n",
        "                    \"Indicator_1\": indicator_i,\n",
        "                    \"Ind_Code_2\": ind_code_j,\n",
        "                    \"Targets and Indicators_2\": target_indicator_j,\n",
        "                    \"Indicator_2\": indicator_j,\n",
        "                    \"Similarity_Score\": score\n",
        "                })\n",
        "\n",
        "        similarity_df = pd.DataFrame(similarity_data)\n",
        "\n",
        "        # Sort by similarity score in descending order to see the most similar pairs first\n",
        "        similarity_df = similarity_df.sort_values(by=\"Similarity_Score\", ascending=False)\n",
        "\n",
        "\n",
        "        # Save the similarity score results\n",
        "        similarity_output_path = \"/content/Similarlity_Score_Revised.xlsx\"\n",
        "        # Include the requested columns: \"Ind_Code\", \"Targets and Indicators\", \"Indicator\" for each pair\n",
        "        similarity_df = similarity_df[['Ind_Code_1', 'Targets and Indicators_1', 'Indicator_1', 'Ind_Code_2', 'Targets and Indicators_2', 'Indicator_2', 'Similarity_Score']]\n",
        "        similarity_df.to_excel(similarity_output_path, index=False)\n",
        "        print(f\"Similarity scores saved to: {similarity_output_path}\")\n",
        "    else:\n",
        "        print(\"Could not proceed with similarity calculation as 'Ind_Code' column was not found in the cleaned DataFrame.\")\n",
        "else:\n",
        "    print(\"DataFrame was not loaded due to file error. Skipping further processing.\")"
      ]
    }
  ]
}